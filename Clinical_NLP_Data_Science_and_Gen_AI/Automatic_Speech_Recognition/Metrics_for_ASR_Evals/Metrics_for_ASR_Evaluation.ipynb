{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics for ASR Evaluation\n",
        "* Notebook by Adam Lang\n",
        "* Date: 5/5/2025\n",
        "\n",
        "# Overview\n",
        "* This notebook contains code examples of various approaches to evaluating automatic speech recognition."
      ],
      "metadata": {
        "id": "KZSNcaY3r5Pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Confidence-based evaluation\n",
        "* Many speech recognition models output a confidence score or probability distribution over possible transcripts.\n",
        "* If your model provides this information, you can use it to estimate the reliability of the ASR transcript.\n",
        "* You can threshold the confidence scores to filter out low-confidence transcripts or words.\n",
        "* In PyTorch, you can access the output probabilities of your model and compute the confidence scores.\n",
        "* For example, if your model outputs a tensor logits with shape (batch_size, sequence_length, vocab_size), you can compute the confidence scores as seen below."
      ],
      "metadata": {
        "id": "hjFjeWqDsFgJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9-vauKqr2Ty"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logits = ...  # output of your speech recognition model\n",
        "probs = F.softmax(logits, dim=-1)  # compute probabilities\n",
        "confidence_scores = probs.max(dim=-1).values  # compute confidence scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Language model-based evaluation -- Perplexity\n",
        "* You can use any language model (LM) to evaluate the **fluency and coherence** of the generated transcripts.\n",
        "* The concept is that a well-formed transcript should have a **low perplexity** under a LM.\n",
        "* You can use a pre-trained LM like BERT or a dedicated speech recognition LM.\n",
        "  * Note: Depending upon the size of the transcript, consider using an LM with a max_sequence > 512 (e.g. don't use BERT base), use ModernBERT instead. Otherwise you will have to chunk the text first or truncate inputs.\n",
        "* In PyTorch, you can use the transformers library to load a pre-trained LM and compute the perplexity of your transcripts as seen in the code below."
      ],
      "metadata": {
        "id": "Y8NF5JUdsQNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel ## can use any model\n",
        "\n",
        "## load  model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "## load transcript\n",
        "transcript = ...  # your transcript text\n",
        "\n",
        "## tokenize transcript\n",
        "inputs = tokenizer.encode_plus(transcript, return_tensors='pt')\n",
        "\n",
        "## send tokenized inputs to model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "## calculate loss\n",
        "loss = torch.nn.CrossEntropyLoss()(outputs.last_hidden_state[:, :-1, :], inputs['input_ids'][:, 1:])\n",
        "\n",
        "## calculate perplexity\n",
        "perplexity = torch.exp(loss)"
      ],
      "metadata": {
        "id": "-UsaD0HBsjzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. SemDistance\n",
        "* SemDist (Semantic Distance) is a metric used in speech recognition (ASR) to assess the semantic similarity between the reference (human) transcription and the hypothesis (system) transcription.\n",
        "* It measures the distance between these two sentences in an embedding space, typically using pre-trained language models like RoBERTa.\n",
        "* **A Lower SemDist scores indicate a greater semantic similarity between the reference and hypothesis.**"
      ],
      "metadata": {
        "id": "l5vtcvydtj8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Load model from hugging face"
      ],
      "metadata": {
        "id": "jOzPCS5-tyaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "## Load model and tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "6h-eQhsytsUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Preprocess the reference and hypothesis transcripts\n",
        "* Preprocess the reference and hypothesis transcripts by tokenizing them and converting them to PyTorch tensors.\n",
        "* Important Distinctions\n",
        "\n",
        "1. **Hypothesis**\n",
        "  * The hypothesis is the transcription output generated by the speech recognition model.\n",
        "2. **Reference**\n",
        "  * The reference is typically considered to be the ground truth or the \"correct\" transcription, usually obtained through human annotation or transcription.\n",
        "  * The reference is used as a benchmark to evaluate the performance of the speech recognition model."
      ],
      "metadata": {
        "id": "hz5U0Gqpt6WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to preprocess transcript\n",
        "def preprocess_transcript(transcript):\n",
        "    inputs = tokenizer(transcript, return_tensors='pt')\n",
        "    return inputs\n",
        "\n",
        "## text reference vs. hypothesis\n",
        "reference = \"This is the reference transcript.\"\n",
        "hypothesis = \"This is the hypothesis transcript.\"\n",
        "\n",
        "\n",
        "## preprocess both\n",
        "reference_inputs = preprocess_transcript(reference)\n",
        "hypothesis_inputs = preprocess_transcript(hypothesis)"
      ],
      "metadata": {
        "id": "s1DeHU_OuB3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Compute sentence embeddings\n",
        "* Compute sentence embeddings for the reference and hypothesis transcripts using the pre-trained `RoBERTa` model.\n",
        "* Note: An alternative to this would be to use a different external embedding model from `SentenceTransformers`."
      ],
      "metadata": {
        "id": "O1-LxGmbuPNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Compute embeddings using RoBERTa model\n",
        "def compute_sentence_embedding(inputs):\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :]  # Use the [CLS] token representation\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "## Get embeddings\n",
        "reference_embedding = compute_sentence_embedding(reference_inputs)\n",
        "hypothesis_embedding = compute_sentence_embedding(hypothesis_inputs)"
      ],
      "metadata": {
        "id": "O4t_rp8ruWv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Calculate SemDist metric\n",
        "* Calculate the SemDist metric by computing the cosine distance or L2 distance between the reference and hypothesis sentence embeddings."
      ],
      "metadata": {
        "id": "ZsSnkG3juotG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to calculate semdist metric\n",
        "def calculate_semdist(embedding1, embedding2):\n",
        "    # Cosine distance\n",
        "    cosine_similarity = torch.nn.CosineSimilarity(dim=1)\n",
        "    semdist = 1 - cosine_similarity(embedding1, embedding2)\n",
        "    return semdist.item()\n",
        "\n",
        "## use function\n",
        "semdist = calculate_semdist(reference_embedding, hypothesis_embedding)\n",
        "print(\"SemDist:\", semdist)"
      ],
      "metadata": {
        "id": "0W-cooU5uuWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Instead, you can also use the L2 distance metric below"
      ],
      "metadata": {
        "id": "TuQvJZ8gu2vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## L2 semdistance\n",
        "def calculate_semdist_l2(embedding1, embedding2):\n",
        "    # L2 distance\n",
        "    l2_distance = torch.nn.PairwiseDistance(p=2)\n",
        "    semdist = l2_distance(embedding1, embedding2)\n",
        "    return semdist.item()\n",
        "\n",
        "## If using semdist_l2 instead\n",
        "semdist_l2 = calculate_semdist_l2(reference_embedding, hypothesis_embedding)\n",
        "print(\"SemDist (L2):\", semdist_l2)"
      ],
      "metadata": {
        "id": "cG-uN5olu5Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Script\n",
        "* If you want to run this as a `.py` script instead"
      ],
      "metadata": {
        "id": "Ovh5G1U0vDuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "## make dir\n",
        "os.makedirs('name_of_dir/')"
      ],
      "metadata": {
        "id": "-Rypx5GwvLNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SemDist/name_of_dir.py\n",
        "\n",
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "## Preprocess transcript function\n",
        "def preprocess_transcript(transcript, tokenizer):\n",
        "    inputs = tokenizer(transcript, return_tensors='pt')\n",
        "    return inputs\n",
        "\n",
        "## Compute embeddings functions\n",
        "def compute_sentence_embedding(inputs, model):\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :]  # Use the [CLS] token representation\n",
        "    return embeddings\n",
        "\n",
        "## Calculate semdist function\n",
        "def calculate_semdist(embedding1, embedding2):\n",
        "    cosine_similarity = torch.nn.CosineSimilarity(dim=1)\n",
        "    semdist = 1 - cosine_similarity(embedding1, embedding2)\n",
        "    return semdist.item()\n",
        "\n",
        "## Load tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "\n",
        "## Init reference and hypothesis transcripts\n",
        "reference = \"This is the reference transcript.\"\n",
        "hypothesis = \"This is the hypothesis transcript.\"\n",
        "\n",
        "\n",
        "## Feed ref and hypoth to preprocess_transcript function\n",
        "reference_inputs = preprocess_transcript(reference, tokenizer)\n",
        "hypothesis_inputs = preprocess_transcript(hypothesis, tokenizer)\n",
        "\n",
        "\n",
        "## Get embeddings for both transcripts\n",
        "reference_embedding = compute_sentence_embedding(reference_inputs, model)\n",
        "hypothesis_embedding = compute_sentence_embedding(hypothesis_inputs, model)\n",
        "\n",
        "\n",
        "## Calculate SemDist\n",
        "semdist = calculate_semdist(reference_embedding, hypothesis_embedding)\n",
        "print(\"SemDist:\", semdist)"
      ],
      "metadata": {
        "id": "0XBBdpFLvG15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# No Ground Truth or Reference Transcripts?!\n",
        "\n",
        "* If you don't have a ground truth or reference transcription, you won't be able to directly compute the SemDist metric, as it requires **both the hypothesis and the reference.**\n",
        "\n",
        "* However, there are a few potential workarounds or alternatives:\n",
        "\n",
        "1. **Pseudo-labeling**\n",
        "  * You can use another speech recognition model or a more advanced language model to generate a pseudo-reference transcription.\n",
        "  * This can be used as a substitute for the ground truth.\n",
        "\n",
        "\n",
        "2. **Unsupervised evaluation**\n",
        "  * You can use other unsupervised evaluation metrics that don't require a ground truth, such as:\n",
        "    * **Confidence-based evaluation** - Confidence scores from the ASR model you are using\n",
        "    * **Language model-based evaluation** - RoBERTa Perplexity example above.\n",
        "    * **Word or Sentence embedding-based evaluation** -- see example below\n",
        "\n",
        "4. **Comparing multiple hypotheses**\n",
        "  * If you have multiple speech recognition models or systems, you can compare their outputs (hypotheses) against each other, even without a ground truth.\n",
        "  * This can help you evaluate the relative performance of the models.\n",
        "\n",
        "5. **Self-supervised learning**\n",
        "  * You can use self-supervised learning techniques, where the model is trained on unlabeled data and learns to predict its own outputs or representations.\n",
        "  * A good example of this would be Chain-of-Thought Prompting with or without In-Context Learning.\n",
        "\n",
        "6. **Unsupervised metrics**\n",
        "  * There are other unsupervised metrics that can be used to evaluate the quality of speech recognition outputs, such as:\n",
        "    * perplexity\n",
        "    * entropy\n"
      ],
      "metadata": {
        "id": "uXE3KqfkwOdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1 - Semantic Coherence -- using Word Embeddings"
      ],
      "metadata": {
        "id": "1CgP08HXxjnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 -  Load pre-trained word or sentence embeddings\n",
        "* You can use pre-trained word embeddings like Word2Vec or GloVe, or encoder based models from HuggingFace (e.g. SentenceTransformers, MixedBread, anything from the MTEB benchmark on HuggingFace).\n",
        "* For this example, we'll use GloVe embeddings but it would probably make more sense to consider using encoder based embeddings such as most SentenceTransformer models."
      ],
      "metadata": {
        "id": "YW_QgzXVx1d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "glove_dict = {}\n",
        "with open(glove_file, 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        glove_dict[values[0]] = np.array(values[1:], dtype='float32')\n",
        "\n",
        "# Create a PyTorch tensor for the word embeddings\n",
        "embedding_dim = 100\n",
        "word_embeddings = torch.zeros((len(glove_dict), embedding_dim))\n",
        "word_to_idx = {}\n",
        "idx = 0\n",
        "for word, vector in glove_dict.items():\n",
        "    word_to_idx[word] = idx\n",
        "    word_embeddings[idx] = torch.from_numpy(vector)\n",
        "    idx += 1"
      ],
      "metadata": {
        "id": "4YE6tBhzxipH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 - Preprocess the transcript\n",
        "* Preprocess the transcript by tokenizing it into individual words and converting them to indices in the word embedding dictionary.\n",
        "* Or if you used an encoder model, then use that tokenizer obviously."
      ],
      "metadata": {
        "id": "8MuZMip7yR-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## preprocess transcript if using word embeddings.\n",
        "def preprocess_transcript(transcript):\n",
        "    words = transcript.lower().split()\n",
        "    indices = []\n",
        "    for word in words:\n",
        "        if word in word_to_idx:\n",
        "            indices.append(word_to_idx[word])\n",
        "    return torch.tensor(indices)\n",
        "\n",
        "## load transcript and preprocess\n",
        "transcript = \"This is an example transcript.\"\n",
        "indices = preprocess_transcript(transcript)"
      ],
      "metadata": {
        "id": "oYvUhIwlybGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 - Compute Semantic coherence\n",
        "* To evaluate semantic coherence, you can use various metrics such as:\n",
        "\n",
        "1. **Average cosine similarity**\n",
        "  * Compute the cosine similarity between consecutive words in the transcript and average them.\n",
        "  * Note: Another method would be to summarize the transcript using an encoder-decoder model like BART or T5, then compare the cosine similarity.\n",
        "  * Note: Another method would be to look at rolling window cosine similarities.\n",
        "\n",
        "2. **Word mover's distance**\n",
        "  * Compute the word mover's distance between the transcript and a reference text (e.g., a pseudo-ground truth transcript).\n",
        "\n",
        "\n",
        "Here's an example of computing **average cosine similarity**:"
      ],
      "metadata": {
        "id": "lSYX_zMyyisq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## function to compute avg cosine similarity using word embeddings -- modify if using other embedding model\n",
        "def compute_average_cosine_similarity(indices, word_embeddings):\n",
        "    similarities = []\n",
        "    for i in range(len(indices) - 1):\n",
        "        word1 = word_embeddings[indices[i]]\n",
        "        word2 = word_embeddings[indices[i + 1]]\n",
        "        similarity = torch.nn.CosineSimilarity(dim=0)(word1, word2)\n",
        "        similarities.append(similarity.item())\n",
        "    return np.mean(similarities)\n",
        "\n",
        "## apply function\n",
        "average_similarity = compute_average_cosine_similarity(indices, word_embeddings)\n",
        "print(\"Average cosine similarity:\", average_similarity)"
      ],
      "metadata": {
        "id": "wg5BzqhHy-e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3b - Using PyTorch to compute semantic coherence\n",
        "* You can also use PyTorch to compute semantic coherence by defining a custom module."
      ],
      "metadata": {
        "id": "KUnyc7RJzLWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Customized Semantic Coherence\n",
        "class SemanticCoherence(torch.nn.Module):\n",
        "    def __init__(self, word_embeddings):\n",
        "        super(SemanticCoherence, self).__init__()\n",
        "        self.word_embeddings = word_embeddings\n",
        "\n",
        "    def forward(self, indices):\n",
        "        embeddings = self.word_embeddings[indices]\n",
        "        similarities = []\n",
        "        for i in range(embeddings.shape[0] - 1):\n",
        "            word1 = embeddings[i]\n",
        "            word2 = embeddings[i + 1]\n",
        "            similarity = torch.nn.CosineSimilarity(dim=0)(word1, word2)\n",
        "            similarities.append(similarity)\n",
        "        return torch.mean(torch.stack(similarities))\n",
        "\n",
        "\n",
        "## run metrics and get results\n",
        "semantic_coherence = SemanticCoherence(word_embeddings)\n",
        "average_similarity = semantic_coherence(indices)\n",
        "print(\"Average cosine similarity:\", average_similarity.item())"
      ],
      "metadata": {
        "id": "I2fdwVG2zRC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2 - Semantic Coherence using SentenceTransformers\n",
        "* Same as above but using SBERT model instead."
      ],
      "metadata": {
        "id": "8xUFwlp8z5dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load a Sentence Transformer model -- pick model of choice\n",
        "## Consider max_sequence_length and dimensions of the transcript\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Define a function to compute the semantic coherence\n",
        "def compute_semantic_coherence(sentences):\n",
        "    # Compute the sentence embeddings\n",
        "    embeddings = model.encode(sentences)\n",
        "\n",
        "    # Compute the cosine similarity between consecutive sentences\n",
        "    similarities = []\n",
        "    for i in range(len(sentences) - 1):\n",
        "        embedding1 = embeddings[i]\n",
        "        embedding2 = embeddings[i + 1]\n",
        "        similarity = util.cos_sim(embedding1, embedding2)\n",
        "        similarities.append(similarity.item())\n",
        "\n",
        "    # Compute the average semantic coherence\n",
        "    average_similarity = np.mean(similarities)\n",
        "    return average_similarity\n",
        "\n",
        "# Define a transcript\n",
        "transcript = \"This is an example transcript. The transcript is used to demonstrate the calculation of semantic coherence. The semantic coherence is a measure of how well the sentences in the transcript are related to each other.\"\n",
        "\n",
        "# Split the transcript into sentences\n",
        "sentences = transcript.split('. ')\n",
        "\n",
        "# Compute the semantic coherence\n",
        "semantic_coherence = compute_semantic_coherence(sentences)\n",
        "print(\"Semantic Coherence:\", semantic_coherence)\n",
        "\n",
        "# Define a function to compute the average semantic distance\n",
        "def compute_average_semantic_distance(sentences):\n",
        "    # Compute the sentence embeddings\n",
        "    embeddings = model.encode(sentences)\n",
        "\n",
        "    # Compute the cosine distance between consecutive sentences\n",
        "    distances = []\n",
        "    for i in range(len(sentences) - 1):\n",
        "        embedding1 = embeddings[i]\n",
        "        embedding2 = embeddings[i + 1]\n",
        "        distance = 1 - util.cos_sim(embedding1, embedding2).item()\n",
        "        distances.append(distance)\n",
        "\n",
        "    # Compute the average semantic distance\n",
        "    average_distance = np.mean(distances)\n",
        "    return average_distance\n",
        "\n",
        "# Compute the average semantic distance\n",
        "average_semantic_distance = compute_average_semantic_distance(sentences)\n",
        "print(\"Average Semantic Distance:\", average_semantic_distance)"
      ],
      "metadata": {
        "id": "KQ9ihl0r0BSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 3 - Using an LLM for Speech\n",
        "* This example uses `SpeechLLM`.\n",
        "* `SpeechLLM is a multi-modal Language Model (LLM) specifically trained to analyze and predict metadata from a speaker's turn in a conversation.\n",
        "* This advanced model integrates a speech encoder to transform speech signals into meaningful speech representations. These embeddings, combined with text instructions, are then processed by the LLM to generate predictions.\n",
        "\n",
        "The model inputs an speech audio file of 16 KHz and predicts the following:\n",
        "```\n",
        "SpeechActivity : if the audio signal contains speech (True/False)\n",
        "Transcript : ASR transcript of the audio\n",
        "Gender of the speaker (Female/Male)\n",
        "Age of the speaker (Young/Middle-Age/Senior)\n",
        "Accent of the speaker (Africa/America/Celtic/Europe/Oceania/South-Asia/South-East-Asia)\n",
        "Emotion of the speaker (Happy/Sad/Anger/Neutral/Frustrated)\n",
        "```\n",
        "* See the repo here: https://github.com/skit-ai/SpeechLLM\n",
        "\n",
        "### General Concept of what you could do with this\n",
        "1. Use `SpeechLLM` to generate a transcript using your audio file(s).\n",
        "2. Then use metrics/methods discussed in this notebook or in the repo to compare the transcription results."
      ],
      "metadata": {
        "id": "M01_sYrH0RcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## sample code of how to run SpeechLLM assuming you have an audio file\n",
        "# Load model directly from huggingface\n",
        "from transformers import AutoModel\n",
        "model = AutoModel.from_pretrained(\"skit-ai/speechllm-2B\", trust_remote_code=True)\n",
        "\n",
        "model.generate_meta(\n",
        "    audio_path=\"path-to-audio.wav\", #16k Hz, mono\n",
        "    audio_tensor=torchaudio.load(\"path-to-audio.wav\")[1], # [Optional] either audio_path or audio_tensor directly\n",
        "    instruction=\"Give me the following information about the audio [SpeechActivity, Transcript, Gender, Emotion, Age, Accent]\",\n",
        "    max_new_tokens=500,\n",
        "    return_special_tokens=False\n",
        ")\n",
        "\n",
        "# Model Generation\n",
        "'''\n",
        "{\n",
        "  \"SpeechActivity\" : \"True\",\n",
        "  \"Transcript\": \"Yes, I got it. I'll make the payment now.\",\n",
        "  \"Gender\": \"Female\",\n",
        "  \"Emotion\": \"Neutral\",\n",
        "  \"Age\": \"Young\",\n",
        "  \"Accent\" : \"America\",\n",
        "}\n",
        "'''"
      ],
      "metadata": {
        "id": "82g_ZWuU087T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "* Then after you get the results you can run comparison using metrics and methods above or in repo to compare this as a reference vs. hypothesis transcript."
      ],
      "metadata": {
        "id": "s5DvCOsZ1IsW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dc7IIzxu1QCn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}